name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install black flake8 mypy

      - name: Check code formatting (black)
        run: black --check src/ tests/

      - name: Check import sorting (isort)
        run: isort --check-only --settings-path pyproject.toml src/ tests/

      - name: Lint with flake8
        run: flake8 src/ tests/ --count --show-source --statistics

      - name: Validate type-ignore comments
        run: python3 scripts/validate_type_ignores.py

      - name: Type check with mypy
        run: mypy src/ --config-file pyproject.toml

  workflow-lint:
    name: Workflow Lint (actionlint)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Lint workflows
        uses: rhysd/actionlint@v1.7.10
        with:
          args: ".github/workflows/ci.yml .github/workflows/deploy-staging.yml .github/workflows/deploy-production.yml"

  config-failfast-proof:
    name: ADR-0002 Fail-Fast Proof
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run fail-fast proof tests (BLOCKING)
        run: |
          echo "=== ADR-0002 Fail-Fast Proof (BLOCKING) ==="
          pytest tests/test_config_failfast.py -v
          echo ""
          echo "✅ Fail-fast proof passed: Production mode fails fast for unsafe config"

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run unit tests
        run: |
          pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=term --junit-xml=junit-unit.xml

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unit
          name: codecov-umbrella
        if: always()

      - name: Upload JUnit XML (Stage 2.0)
        uses: actions/upload-artifact@v4
        with:
          name: junit-unit-tests
          path: junit-unit.xml
          retention-days: 30
        if: always()

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: quality_governance_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Alembic migrations
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
        run: |
          alembic upgrade head
          echo "✅ Migrations applied successfully using Postgres context"

      - name: Validate quarantine policy
        run: |
          python3 scripts/validate_quarantine.py

      - name: Enforce quarantine policy (BLOCKING)
        run: |
          echo "=== QUARANTINE POLICY ENFORCEMENT (BLOCKING) ==="
          python3 scripts/report_test_quarantine.py
          echo ""
          echo "✅ Quarantine policy enforcement passed"

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
        run: |
          pytest tests/integration/ -v --cov=src --cov-report=xml --cov-report=term --junit-xml=junit-integration.xml

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: integration
          name: codecov-umbrella
        if: always()

      - name: Upload JUnit XML (Stage 2.0)
        uses: actions/upload-artifact@v4
        with:
          name: junit-integration-tests
          path: junit-integration.xml
          retention-days: 30
        if: always()

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pip-audit bandit
          pip install -r requirements.txt

      - name: Validate security waivers (BLOCKING)
        run: |
          echo "=== Security Waiver Validation (BLOCKING) ==="
          python3 scripts/validate_security_waivers.py
          echo ""

      - name: Security linting with Bandit (BLOCKING on High severity)
        run: |
          echo "=== Bandit: Security Linting (BLOCKING on High) ==="
          bandit -r src/ -ll -f screen
          echo ""
          echo "✅ Bandit passed: No High severity issues found"

  build-check:
    name: Build Check
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify application starts
        env:
          DATABASE_URL: sqlite+aiosqlite:///./test.db
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          python -c "from src.main import app; print('✅ Application imports successfully')"

  ci-security-covenant:
    name: CI Security Covenant (Stage 2.0)
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Validate CI security covenant (BLOCKING)
        run: |
          echo "=== Stage 2.0: CI Security Covenant Validation (BLOCKING) ==="
          python3 scripts/validate_ci_security_covenant.py
          echo ""
          echo "✅ CI security covenant validation passed"

  smoke-tests:
    name: Smoke Tests (CRITICAL)
    runs-on: ubuntu-latest
    needs: [build-check]
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: quality_governance_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Alembic migrations
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
        run: |
          alembic upgrade head

      - name: Run Smoke Tests (BLOCKING)
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          echo "=== SMOKE TESTS - CRITICAL (BLOCKING) ==="
          pytest tests/smoke/ -v --tb=short -x --junit-xml=junit-smoke.xml
          echo ""
          echo "✅ Smoke tests passed: Core functionality verified"

      - name: Generate Deploy-Proof Evidence (Stage 4)
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          echo "=== DEPLOY-PROOF EVIDENCE GENERATION ===" | tee deploy-proof.txt
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" | tee -a deploy-proof.txt
          echo "Commit: ${{ github.sha }}" | tee -a deploy-proof.txt
          echo "" | tee -a deploy-proof.txt
          
          # 1. Migration Proof
          echo "--- MIGRATION PROOF ---" | tee -a deploy-proof.txt
          EXPECTED_HEAD=$(grep -r "^revision = " alembic/versions/ | grep -v down_rev | tail -1 | sed "s/.*revision = '\([^']*\)'.*/\1/" || echo "unknown")
          echo "Expected head revision: $EXPECTED_HEAD" | tee -a deploy-proof.txt
          
          CURRENT_REV=$(alembic current 2>&1 | tail -1 || echo "unknown")
          echo "Current DB revision: $CURRENT_REV" | tee -a deploy-proof.txt
          
          if echo "$CURRENT_REV" | grep -q "$EXPECTED_HEAD"; then
            echo "✅ Migration proof: PASS - DB at expected head" | tee -a deploy-proof.txt
          else
            echo "⚠️ Migration proof: CHECK - Verify revision alignment" | tee -a deploy-proof.txt
          fi
          echo "" | tee -a deploy-proof.txt
          
          # 2. Security Proof (run app in background for tests)
          echo "--- SECURITY PROOF ---" | tee -a deploy-proof.txt
          
          # Start app in background
          uvicorn src.main:app --host 127.0.0.1 --port 8765 &
          APP_PID=$!
          sleep 5
          
          # Health check
          HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:8765/healthz || echo "000")
          echo "Health check (/healthz): $HEALTH_STATUS" | tee -a deploy-proof.txt
          if [ "$HEALTH_STATUS" = "200" ]; then
            echo "✅ Health check: PASS" | tee -a deploy-proof.txt
          else
            echo "❌ Health check: FAIL" | tee -a deploy-proof.txt
          fi
          
          # Auth required check
          AUTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:8765/api/v1/incidents/ || echo "000")
          echo "Auth required (/api/v1/incidents/): $AUTH_STATUS" | tee -a deploy-proof.txt
          if [ "$AUTH_STATUS" = "401" ]; then
            echo "✅ Auth enforcement: PASS" | tee -a deploy-proof.txt
          else
            echo "❌ Auth enforcement: FAIL (expected 401)" | tee -a deploy-proof.txt
          fi
          
          # Rate limit headers check
          RATE_HEADERS=$(curl -sI http://127.0.0.1:8765/api/v1/incidents/ 2>&1 | grep -i "x-ratelimit" || echo "none")
          echo "Rate limit headers: $RATE_HEADERS" | tee -a deploy-proof.txt
          if echo "$RATE_HEADERS" | grep -qi "x-ratelimit"; then
            echo "✅ Rate limiting: PASS" | tee -a deploy-proof.txt
          else
            echo "⚠️ Rate limiting: CHECK (headers may be absent in test)" | tee -a deploy-proof.txt
          fi
          
          # Stop app
          kill $APP_PID 2>/dev/null || true
          
          echo "" | tee -a deploy-proof.txt
          echo "=== DEPLOY-PROOF EVIDENCE COMPLETE ===" | tee -a deploy-proof.txt

      - name: Upload Deploy-Proof Evidence
        uses: actions/upload-artifact@v4
        with:
          name: deploy-proof-evidence
          path: deploy-proof.txt
          retention-days: 90
        if: always()

      - name: Upload Smoke Test Results
        uses: actions/upload-artifact@v4
        with:
          name: junit-smoke-tests
          path: junit-smoke.xml
          retention-days: 30
        if: always()

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [smoke-tests]
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: quality_governance_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Alembic migrations
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
        run: |
          alembic upgrade head

      - name: Run E2E Tests
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          echo "=== E2E TESTS - Full User Journeys ==="
          # Run tests and capture output
          pytest tests/e2e/ -v --tb=short --junit-xml=junit-e2e.xml 2>&1 | tee e2e-output.txt
          PYTEST_EXIT_CODE=${PIPESTATUS[0]}
          
          # Extract passed count from pytest output
          E2E_PASSED=$(grep -oP '\d+(?= passed)' e2e-output.txt | tail -1 || echo "0")
          E2E_SKIPPED=$(grep -oP '\d+(?= skipped)' e2e-output.txt | tail -1 || echo "0")
          
          echo ""
          echo "=== E2E BASELINE CHECK ==="
          echo "E2E Passed: ${E2E_PASSED:-0}"
          echo "E2E Skipped: ${E2E_SKIPPED:-0}"
          echo "E2E Baseline: 47 (Phase 4)"
          echo "E2E Minimum: 20"
          echo ""
          
          # Check minimum pass threshold
          if [ "${E2E_PASSED:-0}" -lt 20 ]; then
            echo "❌ E2E MINIMUM PASS GATE FAILED"
            echo "   Passed (${E2E_PASSED:-0}) < Minimum (20)"
            echo "   Tests may have regressed or been incorrectly skipped."
            exit 1
          fi
          
          # Check baseline regression (10% tolerance)
          MIN_ACCEPTABLE=27  # 90% of 31
          if [ "${E2E_PASSED:-0}" -lt "${MIN_ACCEPTABLE}" ]; then
            echo "⚠️ E2E BASELINE REGRESSION WARNING"
            echo "   Passed (${E2E_PASSED:-0}) < 90% of baseline (31)"
            echo "   Minimum acceptable: ${MIN_ACCEPTABLE}"
          fi
          
          echo "✅ E2E tests completed: ${E2E_PASSED:-0} passed, ${E2E_SKIPPED:-0} skipped"
          
          # Propagate original pytest exit code
          exit "${PYTEST_EXIT_CODE}"

      - name: Upload E2E Test Results
        uses: actions/upload-artifact@v4
        with:
          name: junit-e2e-tests
          path: junit-e2e.xml
          retention-days: 30
        if: always()

  uat-tests:
    name: UAT Tests (User Acceptance)
    runs-on: ubuntu-latest
    needs: [smoke-tests]
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: quality_governance_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Alembic migrations
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
        run: |
          alembic upgrade head

      - name: Run UAT Stage 1 Tests (Basic Workflows)
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          echo "=== UAT STAGE 1 - Basic Workflow Tests (50 tests) ==="
          pytest tests/uat/test_stage1_basic_workflows.py -v --tb=short --junit-xml=junit-uat-stage1.xml || true
          echo ""
          echo "UAT Stage 1 completed"

      - name: Run UAT Stage 2 Tests (Sophisticated Workflows)
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          echo "=== UAT STAGE 2 - Sophisticated Workflow Tests (20 tests) ==="
          pytest tests/uat/test_stage2_sophisticated_workflows.py -v --tb=short --junit-xml=junit-uat-stage2.xml || true
          echo ""
          echo "UAT Stage 2 completed"

      - name: UAT Stability Guard (Repeat-Run)
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:testpass@localhost:5432/quality_governance_test
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          echo "=== UAT STABILITY GUARD - Repeat-Run Verification ==="
          echo "Running portal workflow tests 3x to detect async flakiness..."
          echo ""
          
          # Run subset of previously-flaky tests 3 times
          # These tests exercise the async DB path that caused event loop issues
          FLAKY_TESTS=(
            "tests/uat/test_stage1_basic_workflows.py::TestEmployeePortalWorkflows::test_uat_002_submit_complaint_report"
            "tests/uat/test_stage1_basic_workflows.py::TestEmployeePortalWorkflows::test_uat_004_track_report_by_reference"
            "tests/uat/test_stage2_sophisticated_workflows.py::TestMultiStepEntityWorkflows::test_suat_002_complaint_with_status_tracking"
          )
          
          PASS_COUNT=0
          FAIL_COUNT=0
          
          for i in 1 2 3; do
            echo "--- Stability Run $i/3 ---"
            if pytest "${FLAKY_TESTS[@]}" -v --tb=line 2>&1 | tail -5; then
              PASS_COUNT=$((PASS_COUNT + 1))
              echo "✅ Run $i: PASS"
            else
              FAIL_COUNT=$((FAIL_COUNT + 1))
              echo "❌ Run $i: FAIL"
            fi
            echo ""
          done
          
          echo "=== STABILITY RESULTS ==="
          echo "Passed: $PASS_COUNT/3"
          echo "Failed: $FAIL_COUNT/3"
          
          if [ $PASS_COUNT -eq 3 ]; then
            echo "✅ Stability guard: PASS - No flakiness detected"
          elif [ $PASS_COUNT -ge 2 ]; then
            echo "⚠️ Stability guard: WARN - Some flakiness detected ($FAIL_COUNT/3 failed)"
          else
            echo "❌ Stability guard: FAIL - Significant flakiness ($FAIL_COUNT/3 failed)"
          fi

      - name: Generate UAT Summary
        run: |
          echo "=== UAT TEST SUMMARY ===" | tee uat-summary.txt
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" | tee -a uat-summary.txt
          echo "Commit: ${{ github.sha }}" | tee -a uat-summary.txt
          echo "" | tee -a uat-summary.txt
          
          if [ -f junit-uat-stage1.xml ]; then
            STAGE1_TESTS=$(grep -o 'tests="[0-9]*"' junit-uat-stage1.xml | head -1 | grep -o '[0-9]*' || echo "0")
            STAGE1_FAILURES=$(grep -o 'failures="[0-9]*"' junit-uat-stage1.xml | head -1 | grep -o '[0-9]*' || echo "0")
            STAGE1_ERRORS=$(grep -o 'errors="[0-9]*"' junit-uat-stage1.xml | head -1 | grep -o '[0-9]*' || echo "0")
            echo "Stage 1: $STAGE1_TESTS tests, $STAGE1_FAILURES failures, $STAGE1_ERRORS errors" | tee -a uat-summary.txt
          fi
          
          if [ -f junit-uat-stage2.xml ]; then
            STAGE2_TESTS=$(grep -o 'tests="[0-9]*"' junit-uat-stage2.xml | head -1 | grep -o '[0-9]*' || echo "0")
            STAGE2_FAILURES=$(grep -o 'failures="[0-9]*"' junit-uat-stage2.xml | head -1 | grep -o '[0-9]*' || echo "0")
            STAGE2_ERRORS=$(grep -o 'errors="[0-9]*"' junit-uat-stage2.xml | head -1 | grep -o '[0-9]*' || echo "0")
            echo "Stage 2: $STAGE2_TESTS tests, $STAGE2_FAILURES failures, $STAGE2_ERRORS errors" | tee -a uat-summary.txt
          fi

      - name: Upload UAT Test Results
        uses: actions/upload-artifact@v4
        with:
          name: uat-test-results
          path: |
            junit-uat-stage1.xml
            junit-uat-stage2.xml
            uat-summary.txt
          retention-days: 30
        if: always()

  openapi-contract-check:
    name: OpenAPI Contract Stability
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate current OpenAPI schema
        env:
          DATABASE_URL: sqlite+aiosqlite:///./test.db
          SECRET_KEY: test-secret-key
          JWT_SECRET_KEY: test-jwt-secret
        run: |
          python -c "
          import json
          from src.main import app
          schema = app.openapi()
          with open('openapi-current.json', 'w') as f:
              json.dump(schema, f, indent=2)
          print('OpenAPI schema generated: openapi-current.json')
          print(f'Paths: {len(schema.get(\"paths\", {}))}')
          print(f'Schemas: {len(schema.get(\"components\", {}).get(\"schemas\", {}))}')
          "

      - name: Check contract compatibility
        run: |
          echo "=== OpenAPI Contract Stability Check ==="
          
          # Check if baseline exists
          if [ -f "openapi-baseline.json" ]; then
            echo "Baseline found, checking compatibility..."
            python scripts/check_openapi_compatibility.py openapi-baseline.json openapi-current.json
          else
            echo "No baseline found - first run, establishing baseline"
            echo "To enable contract checking, commit openapi-baseline.json to the repo"
          fi

      - name: Upload OpenAPI schema
        uses: actions/upload-artifact@v4
        with:
          name: openapi-schema
          path: openapi-current.json
          retention-days: 30
        if: always()

  all-checks:
    name: All Checks Passed
    runs-on: ubuntu-latest
    needs: [code-quality, workflow-lint, config-failfast-proof, unit-tests, integration-tests, security-scan, build-check, ci-security-covenant, smoke-tests, e2e-tests, openapi-contract-check]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Generate gate summary (Stage 2.0)
        run: |
          bash scripts/generate_gate_summary.sh

      - name: Upload gate summary (Stage 2.0)
        uses: actions/upload-artifact@v4
        with:
          name: gate-summary
          path: gate-summary.txt
          retention-days: 90
        if: always()

      - name: All checks passed
        run: |
          echo "✅ All CI checks passed successfully!"
          echo "The code is ready to be merged."
